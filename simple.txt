# Use Debian 12 as the base image for compatibility with Vertex AI
FROM debian:12-slim

# Suppress interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install utilities required by Spark and Python
RUN apt-get update && apt-get install -y \
    procps \
    tini \
    libjemalloc2 \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install OpenJDK 11 (compatible with PySpark 3.5)
RUN apt-get update && apt-get install -y openjdk-11-jdk \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME environment variable
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Install Python 3.11
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-venv \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as the default python3
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Create a virtual environment for Python dependencies
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install requirements from requirements.txt
COPY requirements.txt /opt/spark/work-dir/requirements.txt
# Install dependencies, excluding pyspark to avoid conflicts with Vertex AI's runtime
RUN pip install --no-cache-dir -r /opt/spark/work-dir/requirements.txt --no-deps || \
    pip install --no-cache-dir $(grep -v '^pyspark' /opt/spark/work-dir/requirements.txt)

# Copy the fraud package
COPY fraud /opt/spark/work-dir/fraud

# Install the fraud package as a Python module
RUN pip install /opt/spark/work-dir/fraud

# Enable jemalloc as the default memory allocator (recommended for Spark)
ENV LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2

# Set PYSPARK_PYTHON to point to the virtual environment's Python
ENV PYSPARK_PYTHON=/opt/venv/bin/python

# Create the 'spark' user and group (UID 1099, GID 1099) for Vertex AI
RUN groupadd -g 1099 spark && useradd -u 1099 -g spark -m spark

# Copy your training script (e.g., task.py) to the container
COPY trainer/task.py /opt/spark/work-dir/task.py

# Set permissions for the spark user
RUN chown -R spark:spark /opt/spark/work-dir

# Set the working directory
WORKDIR /opt/spark/work-dir

# Set the entrypoint to run the training script
ENTRYPOINT ["tini", "--", "python", "task.py"]
