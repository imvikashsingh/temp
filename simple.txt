# pipeline.py
import kfp
from kfp.v2 import compiler
from kfp.v2.dsl import pipeline, component, Condition

# --- Configuration ---
PROJECT_ID = "your-gcp-project-id"  # <<< REPLACE WITH YOUR GCP PROJECT ID
REGION = "your-gcp-region"          # <<< REPLACE WITH YOUR GCP REGION, e.g., "us-central1", "europe-west1"
PIPELINE_ROOT = f"gs://your-gcs-bucket-for-artifacts/{kfp.dsl.PIPELINE_JOB_ID}" # <<< REPLACE WITH YOUR GCS BUCKET

# BigQuery details - make sure this dataset exists in your project
BQ_DATASET_ID = "fraud_detection_dataset"
RAW_TRANSACTIONS_TABLE = "raw_transactions"
PROCESSED_TRANSACTIONS_TABLE = "processed_transactions"

# Docker image paths (ensure these match what you pushed to GCR/Artifact Registry)
ETL_IMAGE = f"gcr.io/{PROJECT_ID}/fraud-etl:latest"
MODEL_TRAINING_IMAGE = f"gcr.io/{PROJECT_ID}/fraud-model-training:latest" # Consolidated image

# Define a list of models to train. Each dictionary represents one model.
# Adding a new model is as simple as adding a new dictionary to this list.
MODELS_TO_TRAIN = [
    {"name": "fraud_model_a_logistic_reg", "type": "LOGISTIC_REG"},
    {"name": "fraud_model_b_boosted_tree", "type": "BOOSTED_TREE_CLASSIFIER"},
    {"name": "fraud_model_c_dnn_classifier", "type": "DNN_CLASSIFIER"},
    # Add more models here:
    # {"name": "fraud_model_d_kmeans_clustering", "type": "KMEANS"}, # Example for KMEANS
]

# --- Define Custom Kubeflow Components ---

@component(
    base_image=ETL_IMAGE,
    output_component_file="etl_component.yaml",
)
def etl_step(
    project_id: str,
    dataset_id: str,
    source_table: str,
    processed_table: str,
):
    """
    Kubeflow component for running the ETL process.
    """
    import os
    os.system(
        f"python fraud/etl/data_preparation.py "
        f"--project_id={project_id} "
        f"--dataset_id={dataset_id} "
        f"--source_table={source_table} "
        f"--processed_table={processed_table}"
    )

@component(
    base_image=MODEL_TRAINING_IMAGE,
    output_component_file="model_training_component.yaml",
)
def train_model_step(
    project_id: str,
    dataset_id: str,
    processed_table: str,
    model_name: str,
    model_type: str,
):
    """
    Kubeflow component for training a BigQuery ML model with a specified type.
    """
    import os
    os.system(
        f"python fraud/models/model_training.py "
        f"--project_id={project_id} "
        f"--dataset_id={dataset_id} "
        f"--processed_table={processed_table} "
        f"--model_name={model_name} "
        f"--model_type={model_type}"
    )

# Optional: A final component to indicate pipeline completion
@component
def final_pipeline_step():
    print("All models trained successfully!")

# --- Define the Kubeflow Pipeline DAG ---

@pipeline(
    name="fraud-detection-bigqueryml-matured-parallel-pipeline",
    description="A multi-model fraud detection pipeline with dynamic parallel BigQuery ML training.",
    pipeline_root=PIPELINE_ROOT,
)
def fraud_detection_pipeline(
    project_id: str = PROJECT_ID,
    region: str = REGION,
    bq_dataset_id: str = BQ_DATASET_ID,
    raw_transactions_table: str = RAW_TRANSACTIONS_TABLE,
    processed_transactions_table: str = PROCESSED_TRANSACTIONS_TABLE,
):
    # Step 1: ETL for data preparation
    etl_task = etl_step(
        project_id=project_id,
        dataset_id=bq_dataset_id,
        source_table=raw_transactions_table,
        processed_table=processed_transactions_table,
    )

    # Step 2: Parallel Model Training using with_items
    # This loop dynamically creates a parallel task for each item in MODELS_TO_TRAIN.
    # Each task will run independently and concurrently after the ETL step.
    with kfp.dsl.ParallelFor(MODELS_TO_TRAIN) as model_item:
        train_task = train_model_step(
            project_id=project_id,
            dataset_id=bq_dataset_id,
            processed_table=processed_transactions_table,
            model_name=model_item["name"], # Access dict keys for parameters
            model_type=model_item["type"],
        )
        train_task.after(etl_task) # Each parallel task depends on the ETL step

    # Step 3: Final step that waits for ALL parallel training tasks to complete
    # The `final_pipeline_step` will automatically wait for all tasks in the
    # `ParallelFor` loop that it directly depends on to finish.
    # No need for individual `.after()` calls for each dynamic task.
    final_pipeline_step_task = final_pipeline_step()
    final_pipeline_step_task.after(train_task) # 'train_task' here refers to the *last* task created in the loop,
                                               # but when used with ParallelFor, it correctly implies waiting for all.


# --- Pipeline Compilation and Submission ---
if __name__ == "__main__":
    compiler.Compiler().compile(
        pipeline_func=fraud_detection_pipeline,
        package_path="fraud_detection_pipeline.json",
    )
    print("Pipeline compiled to fraud_detection_pipeline.json")

    # Optional: Submit the pipeline job directly from here (for testing/development)
    # from google.cloud import aiplatform

    # aiplatform.init(project=PROJECT_ID, location=REGION)

    # job = aiplatform.PipelineJob(
    #     display_name="fraud-bqml-matured-parallel-run",
    #     template_path="fraud_detection_pipeline.json",
    #     pipeline_root=PIPELINE_ROOT,
    #     enable_caching=True,
    # )
    # job.run()
    # print(f"Pipeline job submitted. View at: {job.console_uri}")
