# Dockerfile

# Use a base image with Python 3.11 and Java 11 installed for PySpark 3.5
# This image is based on Debian and includes OpenJDK 11, suitable for PySpark 3.5
FROM openjdk:11-jdk-slim-buster

# Install Python 3.11 and pip
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.11 \
    python3-pip \
    python3.11-venv \
    curl \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default python
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Set environment variables for PySpark
ENV SPARK_HOME="/opt/spark"
ENV PATH="${PATH}:${SPARK_HOME}/bin"
ENV JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64" # Adjusted for Java 11

# Create a working directory in the container
WORKDIR /app

# Copy your requirements.txt file into the container
# It's good practice to copy this before copying the rest of the code
# to leverage Docker layer caching if only requirements change.
COPY requirements.txt /app/

# Install Python dependencies from requirements.txt
# Using --no-cache-dir for smaller image size
RUN pip install --no-cache-dir -r /app/requirements.txt

# Copy your entire source code directory into the container
# Assuming your project root has a 'src' directory
COPY src /app/src

# Set the entrypoint for the container
# This command will be executed when the container starts
# The script expects arguments for data-path and model-dir
ENTRYPOINT ["python", "-m", "src.fraud.train"]

# Example of how to build and run locally (for testing):
# docker build -t fraud-detector-trainer .
# docker run -it fraud-detector-trainer --data-path /app/src/fraud/dummy_data.csv --model-dir /tmp/models
# Note: For local testing, you might need to create a dummy_data.csv if you modify load_data_with_pyspark
# to read from a local file instead of generating data.
