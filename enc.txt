# Dockerfile

# Use a base image with Python 3.11 and Java installed for PySpark 3.5
# This image is based on Debian and includes OpenJDK 17, suitable for PySpark 3.5
FROM openjdk:17-jdk-slim-buster

# Install Python 3.11 and pip
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.11 \
    python3-pip \
    python3.11-venv \
    curl \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default python
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1

# Set environment variables for PySpark
ENV SPARK_HOME="/opt/spark"
ENV PATH="${PATH}:${SPARK_HOME}/bin"
ENV JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64" # Adjust if your Java installation path differs

# Install PySpark 3.5.0, scikit-learn, pandas, joblib, and Google Cloud AI Platform SDK
# Using --no-cache-dir for smaller image size
RUN pip install --no-cache-dir \
    pyspark==3.5.0 \
    scikit-learn==1.4.2 \
    pandas==2.2.2 \
    numpy==1.26.4 \
    joblib==1.4.2 \
    google-cloud-aiplatform==1.49.0 \
    google-cloud-storage==2.11.0 # Required for gsutil cp or direct GCS interaction

# Create a working directory in the container
WORKDIR /app

# Copy your entire source code directory into the container
# Assuming your project root has a 'src' directory
COPY src /app/src

# Set the entrypoint for the container
# This command will be executed when the container starts
# The script expects arguments for data-path and model-dir
ENTRYPOINT ["python", "-m", "src.fraud.train"]

# Example of how to build and run locally (for testing):
# docker build -t fraud-detector-trainer .
# docker run -it fraud-detector-trainer --data-path /app/src/fraud/dummy_data.csv --model-dir /tmp/models
# Note: For local testing, you might need to create a dummy_data.csv if you modify load_data_with_pyspark
# to read from a local file instead of generating data.
